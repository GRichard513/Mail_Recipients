{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Email recipient recommendation</h1>\n",
    "\n",
    "<i>Thomas Boudou, Guillaume Richard, Antoine Simoulin</i>\n",
    "\n",
    "<p style=\"text-align: justify\">It was shown that at work, employees frequently forget to include one or more recipient(s) before sending a message. Conversely, it is common that some recipients of a given message were actually not intended to receive the message. To increase productivity and prevent information leakage, the needs for effective <b>email recipient recommendation</b> systems are thus pressing.\n",
    "\n",
    "In this challenge, you are asked to develop such a system, which, given the content and the date of a message, recommends a list of <b>10 recipients ranked by decreasing order of relevance</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "%matplotlib inline\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# do not display warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Functions files are saved in \"src/\" directory.\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "from accuracy_measure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "\n",
    "# load files\n",
    "# Data are saved in \"data/\" directory\n",
    "path_to_data = '../data/'\n",
    "training, training_info, test, test_info, y_df = load_data(path_to_data)\n",
    "\n",
    "# create adress book\n",
    "# /!\\ can take 1-2 min\n",
    "address_books = create_address_books(training, y_df)\n",
    "\n",
    "# join train and test files\n",
    "X_df = join_data(training_info, training)\n",
    "X_sub_df = join_data(test_info, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TF-IDF </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "class TFIDF():\n",
    "    def __init__(self):\n",
    "        self.token_dict = {}\n",
    "        self.tfidf=TfidfVectorizer(tokenizer=None, stop_words='english')\n",
    "\n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[0]):\n",
    "            text = X.body.values[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "            self.token_dict[i] = y\n",
    "\n",
    "        self.tfidf.fit(self.token_dict.values())\n",
    "\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        start_time = time.time()\n",
    "        for i in range(X.shape[0]):\n",
    "            text = X.body.values[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "\n",
    "            self.token_dict[i] = y\n",
    "\n",
    "        X_tfidf = self.tfidf.fit_transform(self.token_dict.values())\n",
    "\n",
    "        print('performed Tf-Idf in %2i seconds.' % (time.time() - start_time))\n",
    "        return X_tfidf\n",
    "\n",
    "    def transform(self, Y):\n",
    "        start_time = time.time()\n",
    "        Y_dict={}\n",
    "        for i in range(Y.shape[0]):\n",
    "            text = Y.body.values[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "\n",
    "            Y_dict[i] = y\n",
    "        Y_tf_idf=self.tfidf.transform(Y_dict.values())\n",
    "\n",
    "        print('performed Tf-Idf in %2i seconds.' % (time.time() - start_time))\n",
    "        return Y_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Useful functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Score vector creation\n",
    "def score_vector(KNN_indices,sender_index,sender_AB,y,cos_dist_mat):\n",
    "    recipient_scores=np.zeros((sum(sender_index),len(sender_AB)+1))\n",
    "    for i in range(sum(sender_index)):\n",
    "        d=np.array(KNN_indices[i])\n",
    "        neigh_mails=y.values[d]#neighbour mails\n",
    "        z=0\n",
    "        for n_mail in neigh_mails:\n",
    "            for rec in n_mail:\n",
    "                if rec in sender_AB:\n",
    "                    j=sender_AB[rec]#index in the score vector\n",
    "                    recipient_scores[i,j]+=cos_dist_mat[i,d[z]]\n",
    "            z=z+1\n",
    "    return recipient_scores\n",
    "\n",
    "#Label creation (from recipient addresses to 0/1 vector)\n",
    "def create_labels(sender_train_is,sender_AB,y_train):\n",
    "    recipient_labels=np.zeros((sum(sender_train_is),len(sender_AB)))\n",
    "    i=0\n",
    "    for rec_list in y_train[sender_train_is]:\n",
    "        for rec in rec_list:\n",
    "            if rec in sender_AB:\n",
    "                j=sender_AB[rec]\n",
    "                recipient_labels[i,j]=1 \n",
    "        i=i+1\n",
    "    return recipient_labels\n",
    "\n",
    "#Complete prediction when <10\n",
    "def complete_prediction(k, sender, address_books, res_temp, K=10):\n",
    "    # k the number of recipients to predict\n",
    "    k_most = [elt[0] for elt in address_books[sender][:K] if elt not in res_temp]\n",
    "    k_most = k_most[:k]\n",
    "    if len(k_most) < k: # sender n'a pas assez de contacts\n",
    "        k_most.extend([0] * (k-len(k_most)))\n",
    "    return k_most\n",
    "\n",
    "#Computes the KNN on the distance matrix\n",
    "def KNN(distance,k=30):\n",
    "    indexes=[]\n",
    "    for d in distance:\n",
    "        indexes.append((-d).argsort()[:k])\n",
    "    return np.array(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Fitting </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Cross-Validation Module--------\n",
      "\n",
      " Beginning of extraction \n",
      " ------------\n",
      "performed Tf-Idf in 16 seconds.\n",
      "performed Tf-Idf in  5 seconds.\n",
      "Extraction done \n",
      " ------------\n",
      "\n",
      " Beginning of prediction \n",
      " ------------\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "End of prediction\n",
      "------------\n",
      "         0 |                   beth.cherry@enform.com | 0.89\n",
      "         1 |                    susan.scott@enron.com | 0.12\n",
      "         2 |                      jean.mrha@enron.com | 0.61\n",
      "         3 |                 stanley.horton@enron.com | 0.16\n",
      "         4 |                sara.shackleton@enron.com | 0.14\n",
      "         5 |                     lynn.blair@enron.com | 0.27\n",
      "         6 |                  chris.dorland@enron.com | 0.17\n",
      "         7 |                    mark.palmer@enron.com | 0.52\n",
      "         8 |                     tim.belden@enron.com | 0.17\n",
      "         9 |                    marie.heard@enron.com | 0.17\n",
      "        10 |               michael.tribolet@enron.com | 0.52\n",
      "        11 |                 phillip.m.love@enron.com | 0.14\n",
      "        12 |                james.d.steffes@enron.com | 0.25\n",
      "        13 |                  sheila.glover@enron.com | 0.48\n",
      "        14 |                     l..nicolay@enron.com | 0.58\n",
      "        15 |                 ginger.dernehl@enron.com | 0.63\n",
      "        16 |                   shona.wilson@enron.com | 0.53\n",
      "        17 |                       c..giron@enron.com | 0.17\n",
      "        18 |                 janel.guerrero@enron.com | 0.57\n",
      "        19 |          nancy.sellers@robertmondavi.com | 0.87\n",
      "        20 |                  chris.germany@enron.com | 0.1\n",
      "        21 |                harry.kingerski@enron.com | 0.48\n",
      "        22 |                    alan.comnes@enron.com | 0.46\n",
      "        23 |               sandra.f.brawner@enron.com | 0.28\n",
      "        24 |                richard.shapiro@enron.com | 0.46\n",
      "        25 |                     jane.tholt@enron.com | 0.14\n",
      "        26 |                  martin.cuilla@enron.com | 0.19\n",
      "        27 |               errol.mclaughlin@enron.com | 0.2\n",
      "        28 |                      l..denton@enron.com | 0.92\n",
      "        29 |                 kevin.m.presto@enron.com | 0.21\n",
      "        30 |                 barry.tycholiz@enron.com | 0.27\n",
      "        31 |                    jason.wolfe@enron.com | 0.39\n",
      "        32 |                  david.forster@enron.com | 0.57\n",
      "        33 |                     wsmith@wordsmith.org | 1.0\n",
      "        34 |             kenneth.thibodeaux@enron.com | 0.87\n",
      "        35 |                       kim.ward@enron.com | 0.2\n",
      "        36 |                stephanie.panus@enron.com | 0.39\n",
      "        37 |                 stacey.w.white@enron.com | 0.72\n",
      "        38 |                   paul.kaufman@enron.com | 0.49\n",
      "        39 |                  andrew.edison@enron.com | 0.39\n",
      "        40 |                   mike.grigsby@enron.com | 0.33\n",
      "        41 |                 mark.greenberg@enron.com | 0.51\n",
      "        42 |                     sally.beck@enron.com | 0.14\n",
      "        43 |        enron_update@concureworkplace.com | 0.24\n",
      "        44 |                julie.armstrong@enron.com | 0.53\n",
      "        45 |             joannie.williamson@enron.com | 0.53\n",
      "        46 |            darrell.schoolcraft@enron.com | 0.47\n",
      "        47 |                  lorna.brennan@enron.com | 0.81\n",
      "        48 |                  karen.buckley@enron.com | 0.5\n",
      "        49 |                tori.kuykendall@enron.com | 0.09\n",
      "        50 |                      eric.bass@enron.com | 0.2\n",
      "        51 |                     scott.neal@enron.com | 0.18\n",
      "        52 |                    mike.carson@enron.com | 0.13\n",
      "        53 |                            alex@pira.com | 1.0\n",
      "        54 |                  becky.spencer@enron.com | 0.57\n",
      "        55 |                  tanya.rohauer@enron.com | 0.59\n",
      "        56 |                 jennifer.thome@enron.com | 0.64\n",
      "        57 |                  michelle.cash@enron.com | 0.17\n",
      "        58 |                  james.derrick@enron.com | 0.49\n",
      "        59 |               larry.f.campbell@enron.com | 0.14\n",
      "        60 |                    andy.zipper@enron.com | 0.18\n",
      "        61 |                    karen.denne@enron.com | 0.55\n",
      "        62 |                     mark.whitt@enron.com | 0.4\n",
      "        63 |                         taylor@enron.com | 0.14\n",
      "        64 |                  dutch.quigley@enron.com | 0.24\n",
      "        65 |                  paul.d.thomas@enron.com | 0.22\n",
      "        66 |                 heather.dunton@enron.com | 0.48\n",
      "        67 |                       jbennett@gmssr.com | 0.56\n",
      "        68 |                      sylvia.hu@enron.com | 0.52\n",
      "        69 |                  john.lavorato@enron.com | 0.21\n",
      "        70 |                      m..forney@enron.com | 0.19\n",
      "        71 |                     liz.taylor@enron.com | 0.51\n",
      "        72 |               stephanie.miller@enron.com | 0.55\n",
      "        73 |                     m..schmidt@enron.com | 0.99\n",
      "        74 |                   paul.y barbo@enron.com | 0.22\n",
      "        75 |                      mary.cook@enron.com | 0.37\n",
      "        76 |                    britt.davis@enron.com | 0.45\n",
      "        77 |                  brian.redmond@enron.com | 0.35\n",
      "        78 |               monika.causholli@enron.com | 0.36\n",
      "        79 |                        vkaminski@aol.com | 0.93\n",
      "        80 |                phillip.platter@enron.com | 0.13\n",
      "        81 |              kathleen.carnahan@enron.com | 0.89\n",
      "        82 |                   megan.parker@enron.com | 0.88\n",
      "        83 |                  jim.schwieger@enron.com | 0.19\n",
      "        84 |                grace.rodriguez@enron.com | 0.46\n",
      "        85 |               christina.valdez@enron.com | 0.78\n",
      "        86 |                 jason.williams@enron.com | 0.25\n",
      "        87 |               marcus.nettelton@enron.com | 0.57\n",
      "        88 |                stephanie.sever@enron.com | 0.66\n",
      "        89 |                       rick.buy@enron.com | 0.16\n",
      "        90 |                     bob.shults@enron.com | 0.44\n",
      "        91 |                     matt.smith@enron.com | 0.1\n",
      "        92 |                    andrea.ring@enron.com | 0.34\n",
      "        93 |                    rahil.jafry@enron.com | 0.52\n",
      "        94 |               hunter.s.shively@enron.com | 0.22\n",
      "        95 |               fletcher.j.sturm@enron.com | 0.07\n",
      "        96 |                christian.yoder@enron.com | 0.42\n",
      "        97 |                   holly.keiser@enron.com | 0.64\n",
      "        98 |               holden.salisbury@enron.com | 0.1\n",
      "        99 |                  peter.keohane@enron.com | 0.29\n",
      "       100 |                 alan.aronowitz@enron.com | 0.4\n",
      "       101 |                kimberly.hillis@enron.com | 0.4\n",
      "       102 |                russell.diamond@enron.com | 0.31\n",
      "       103 |                  suzanne.adams@enron.com | 0.72\n",
      "       104 |                 jonathan.mckay@enron.com | 0.13\n",
      "       105 |                 mark.mcconnell@enron.com | 0.57\n",
      "       106 |                     david.port@enron.com | 0.28\n",
      "       107 |                    cindy.stark@enron.com | 0.28\n",
      "       108 |                 keegan.farrell@enron.com | 0.57\n",
      "       109 |                  john.zufferli@enron.com | 0.11\n",
      "       110 |                     brad.mckay@enron.com | 0.28\n",
      "       111 |                     greg.piper@enron.com | 0.27\n",
      "       112 |                    c..williams@enron.com | 0.42\n",
      "       113 |    schwabalerts.marketupdates@schwab.com | 1.0\n",
      "       114 |                     ben.jacoby@enron.com | 0.55\n",
      "       115 |                 cheryl.johnson@enron.com | 0.89\n",
      "       116 |                    david.portz@enron.com | 0.28\n",
      "       117 |                     mike.maggi@enron.com | 0.57\n",
      "       118 |                 patrice.l.mims@enron.com | 0.11\n",
      "       119 |                lisa.mellencamp@enron.com | 0.19\n",
      "       120 |                    w..cantrell@enron.com | 0.72\n",
      "       121 |                 justin.rostant@enron.com | 0.58\n",
      "       122 |                    amr.ibrahim@enron.com | 0.54\n",
      "       123 |                amy.fitzpatrick@enron.com | 0.56\n",
      "       124 |               joe.stepenovitch@enron.com | 0.54\n",
      "0.422201956892\n",
      "CPU times: user 7min 30s, sys: 8.75 s, total: 7min 39s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#import TFIDF_mod\n",
    "#from TFIDF_mod import TFIDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# splitting data for cross validation\n",
    "skf = ShuffleSplit(n_splits=1, test_size=0.25)\n",
    "\n",
    "print('--------Cross-Validation Module--------')\n",
    "for train_is, test_is in skf.split(y_df):\n",
    "    print('\\n Beginning of extraction \\n ------------')\n",
    "    ############Extraction + TF-IDF############\n",
    "    X_train=X_df.ix[train_is]\n",
    "    y_train = y_df.recipients.loc[train_is].copy()\n",
    "    X_test=X_df.ix[test_is]\n",
    "    y_test = y_df.recipients.loc[test_is].copy()\n",
    "    y_pred=y_test.copy()\n",
    "    \n",
    "    tf_idf = TFIDF()\n",
    "    X_train_TFIDF=tf_idf.fit_transform(X_train)\n",
    "    X_test_TFIDF=tf_idf.transform(X_test)\n",
    "    \n",
    "    print('Extraction done \\n ------------')\n",
    "    print('\\n Beginning of prediction \\n ------------')\n",
    "    ############Prediction############\n",
    "    sender_test = X_test.sender.unique().tolist()\n",
    "    clf={}\n",
    "    count=0\n",
    "    L=len(sender_test)\n",
    "    y_pred=y_test.copy()\n",
    "    \n",
    "    tot_rec_mails={}\n",
    "    for sender in sender_test:\n",
    "        for x in address_books[sender]:\n",
    "            if x[0] in tot_rec_mails:\n",
    "                tot_rec_mails[x[0]]=tot_rec_mails[x[0]]+x[1]\n",
    "            else:\n",
    "                tot_rec_mails[x[0]]=x[1]\n",
    "    for sender in sender_test:\n",
    "        #Isolation of sender's mails\n",
    "        sender_train_is = np.array(X_train.sender == sender)\n",
    "        sender_test_is = np.array(X_test.sender == sender)\n",
    "\n",
    "        ############Feature extraction############\n",
    "        \n",
    "        #Finding the nearest neighbours of sender's mails\n",
    "        cos_dist_mat=cosine_similarity(X_train_TFIDF[sender_train_is])-np.identity(sum(sender_train_is))\n",
    "        cos_dist_mat_test=cosine_similarity(X_test_TFIDF[sender_test_is],X_train_TFIDF[sender_train_is])\n",
    "        #cos_dist_mat=cosine_similarity(X_TFIDF[sender_train_is],X_TFIDF) #to try later\n",
    "        \n",
    "        #KNN\n",
    "        KNN_indices=KNN(cos_dist_mat,k=50)\n",
    "        KNN_indices_test=KNN(cos_dist_mat_test,k=50)\n",
    "\n",
    "        #Sender number in the address book\n",
    "        sender_AB={}\n",
    "        id_to_sender={}\n",
    "        sent_frequency={}\n",
    "        rec_frequency={}\n",
    "        n_mails=float(sum(sender_train_is))\n",
    "        z=0\n",
    "        for x in address_books[sender]:\n",
    "            sender_AB[x[0]]=z\n",
    "            id_to_sender[z]=x[0]\n",
    "            sent_frequency[x[0]]=x[1]/n_mails\n",
    "            rec_frequency[x[0]]=float(x[1])/tot_rec_mails[x[0]]\n",
    "            z=z+1\n",
    "\n",
    "        #Creation of the score vector\n",
    "        recipient_scores=score_vector(KNN_indices,sender_train_is,sender_AB,y_train,cos_dist_mat) \n",
    "\n",
    "        \n",
    "        ############Train############\n",
    "\n",
    "        #Creation of the labels for the classifier\n",
    "        recipient_labels=create_labels(sender_train_is,sender_AB,y_train)\n",
    "\n",
    "        #One classifier per recipient\n",
    "        for rec in sender_AB:\n",
    "            #Adding frequency feature\n",
    "            #recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\n",
    "            x_fit=np.array([[x, sent_frequency[rec], rec_frequency[rec]] for x in recipient_scores.T[sender_AB[rec]]])\n",
    "            key=sender+','+rec\n",
    "            #clf[key]=SVC()\n",
    "            clf[key]=xgb.XGBClassifier(n_estimators=10)\n",
    "            clf[key].fit(x_fit,recipient_labels.T[sender_AB[rec]])\n",
    "\n",
    "\n",
    "        ############Test############\n",
    "\n",
    "        #Creation of the test score vector\n",
    "        recipient_scores=score_vector(KNN_indices_test,sender_test_is,sender_AB,y_train,cos_dist_mat_test) \n",
    "        recipient_labels=np.zeros((sum(sender_test_is),len(sender_AB))).T\n",
    "\n",
    "        #Prediction\n",
    "        pred=0\n",
    "        for rec in sender_AB:\n",
    "            #Adding frequency feature\n",
    "            recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\n",
    "            x_fit=np.array([[x, sent_frequency[rec], rec_frequency[rec]] for x in recipient_scores.T[sender_AB[rec]]])\n",
    "            #Predict\n",
    "            key=sender+','+rec\n",
    "            recipient_labels[sender_AB[rec]]=(clf[key].predict_proba(x_fit)).T[1].T\n",
    "        recipient_labels=recipient_labels.T\n",
    "        #Storage\n",
    "        y_test_pred=[]\n",
    "        for y in recipient_labels:\n",
    "            y_tmp=[]\n",
    "            max_rec=(-y).argsort()[:10]\n",
    "            for rec_id in max_rec:\n",
    "                y_tmp.append(id_to_sender[rec_id])\n",
    "            if len(y_tmp) < 10:\n",
    "                y_tmp.extend(complete_prediction(10-len(y_tmp),sender, address_books, y_tmp))\n",
    "            y_test_pred.append(y_tmp)\n",
    "        y_pred.ix[sender_test_is]=y_test_pred\n",
    "        \n",
    "        if int((count*10)/L)>int(((count-1)*10)/L):\n",
    "            print(round(float(count*100)/L))\n",
    "        count=count+1\n",
    "    print('End of prediction')\n",
    "    print('------------')\n",
    "\n",
    "for train_is, test_is in skf.split(y_df):\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    accuracy = {}\n",
    "    accuracy_freq = {}\n",
    "    accuracy_TOT = 0\n",
    "    for sender in sender_test:\n",
    "        print('%10s | %40s | ' %(sender_test.index(sender), sender), end='')\n",
    "        sender_train_is = np.array(X_train.sender == sender)\n",
    "        sender_test_is = np.array(X_test.sender == sender)\n",
    "        accuracy[sender] = mapk(y_test[sender_test_is], y_pred[sender_test_is])\n",
    "        accuracy_TOT += accuracy[sender]\n",
    "        print(round(accuracy[sender],2))\n",
    "print(accuracy_TOT/len(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Submission </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Cross-Validation Module--------\n",
      "\n",
      " Beginning of extraction \n",
      " ------------\n",
      "performed Tf-Idf in 25 seconds.\n",
      "performed Tf-Idf in  1 seconds.\n",
      "Extraction done \n",
      " ------------\n",
      "\n",
      " Beginning of prediction \n",
      " ------------\n",
      "10 %\n",
      "20 %\n",
      "30 %\n",
      "40 %\n",
      "50 %\n",
      "60 %\n",
      "70 %\n",
      "80 %\n",
      "90 %\n",
      "End of prediction\n",
      "------------\n",
      "CPU times: user 7min 11s, sys: 7.5 s, total: 7min 19s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#import TFIDF_mod\n",
    "#from TFIDF_mod import TFIDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# splitting data for cross validation\n",
    "skf = ShuffleSplit(n_splits=1, test_size=0.25)\n",
    "\n",
    "print('--------Cross-Validation Module--------')\n",
    "for train_is, test_is in skf.split(y_df):\n",
    "    print('\\n Beginning of extraction \\n ------------')\n",
    "    ############Extraction + TF-IDF############\n",
    "    X_train=X_df.copy()\n",
    "    y_train = y_df.recipients.copy()\n",
    "    X_test=X_sub_df\n",
    "    #y_test = y_df.recipients.loc[test_is].copy()\n",
    "    y_pred=pd.Series([[''] for x in X_sub_df.sender])\n",
    "    \n",
    "    tf_idf = TFIDF()\n",
    "    X_train_TFIDF=tf_idf.fit_transform(X_train)\n",
    "    X_test_TFIDF=tf_idf.transform(X_test)\n",
    "    \n",
    "    print('Extraction done \\n ------------')\n",
    "    print('\\n Beginning of prediction \\n ------------')\n",
    "    ############Prediction############\n",
    "    sender_test = X_test.sender.unique().tolist()\n",
    "    clf={}\n",
    "    count=0\n",
    "    L=len(sender_test)\n",
    "    \n",
    "    tot_rec_mails={}\n",
    "    for sender in sender_test:\n",
    "        for x in address_books[sender]:\n",
    "            if x[0] in tot_rec_mails:\n",
    "                tot_rec_mails[x[0]]=tot_rec_mails[x[0]]+x[1]\n",
    "            else:\n",
    "                tot_rec_mails[x[0]]=x[1]\n",
    "    for sender in sender_test:\n",
    "        #Isolation of sender's mails\n",
    "        sender_train_is = np.array(X_train.sender == sender)\n",
    "        sender_test_is = np.array(X_test.sender == sender)\n",
    "\n",
    "        ############Feature extraction############\n",
    "        \n",
    "        #Finding the nearest neighbours of sender's mails\n",
    "        cos_dist_mat=cosine_similarity(X_train_TFIDF[sender_train_is])-np.identity(sum(sender_train_is))\n",
    "        cos_dist_mat_test=cosine_similarity(X_test_TFIDF[sender_test_is],X_train_TFIDF[sender_train_is])\n",
    "        #cos_dist_mat=cosine_similarity(X_TFIDF[sender_train_is],X_TFIDF) #to try later\n",
    "        \n",
    "        #KNN\n",
    "        KNN_indices=KNN(cos_dist_mat,k=50)\n",
    "        KNN_indices_test=KNN(cos_dist_mat_test,k=50)\n",
    "\n",
    "        #Sender number in the address book\n",
    "        sender_AB={}\n",
    "        id_to_sender={}\n",
    "        sent_frequency={}\n",
    "        rec_frequency={}\n",
    "        n_mails=float(sum(sender_train_is))\n",
    "        z=0\n",
    "        for x in address_books[sender]:\n",
    "            sender_AB[x[0]]=z\n",
    "            id_to_sender[z]=x[0]\n",
    "            sent_frequency[x[0]]=x[1]/n_mails\n",
    "            rec_frequency[x[0]]=float(x[1])/tot_rec_mails[x[0]]\n",
    "            z=z+1\n",
    "\n",
    "        #Creation of the score vector\n",
    "        recipient_scores=score_vector(KNN_indices,sender_train_is,sender_AB,y_train,cos_dist_mat) \n",
    "\n",
    "        \n",
    "        ############Train############\n",
    "\n",
    "        #Creation of the labels for the classifier\n",
    "        recipient_labels=create_labels(sender_train_is,sender_AB,y_train)\n",
    "\n",
    "        #One classifier per recipient\n",
    "        for rec in sender_AB:\n",
    "            #Adding frequency feature\n",
    "            #recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\n",
    "            x_fit=np.array([[x, sent_frequency[rec], rec_frequency[rec]] for x in recipient_scores.T[sender_AB[rec]]])\n",
    "            key=sender+','+rec\n",
    "            #clf[key]=SVC()\n",
    "            clf[key]=xgb.XGBClassifier(n_estimators=10)\n",
    "            clf[key].fit(x_fit,recipient_labels.T[sender_AB[rec]])\n",
    "\n",
    "\n",
    "        ############Test############\n",
    "\n",
    "        #Creation of the test score vector\n",
    "        recipient_scores=score_vector(KNN_indices_test,sender_test_is,sender_AB,y_train,cos_dist_mat_test) \n",
    "        recipient_labels=np.zeros((sum(sender_test_is),len(sender_AB))).T\n",
    "\n",
    "        #Prediction\n",
    "        pred=0\n",
    "        for rec in sender_AB:\n",
    "            #Adding frequency feature\n",
    "            recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\n",
    "            x_fit=np.array([[x, sent_frequency[rec], rec_frequency[rec]] for x in recipient_scores.T[sender_AB[rec]]])\n",
    "            #Predict\n",
    "            key=sender+','+rec\n",
    "            recipient_labels[sender_AB[rec]]=(clf[key].predict_proba(x_fit)).T[1].T\n",
    "        recipient_labels=recipient_labels.T\n",
    "        #Storage\n",
    "        y_test_pred=[]\n",
    "        for y in recipient_labels:\n",
    "            y_tmp=[]\n",
    "            max_rec=(-y).argsort()[:10]\n",
    "            for rec_id in max_rec:\n",
    "                y_tmp.append(id_to_sender[rec_id])\n",
    "            if len(y_tmp) < 10:\n",
    "                y_tmp.extend(complete_prediction(10-len(y_tmp),sender, address_books, y_tmp))\n",
    "            y_test_pred.append(y_tmp)\n",
    "        y_pred.ix[sender_test_is]=y_test_pred\n",
    "        \n",
    "        if int((count*10)/L)>int(((count-1)*10)/L):\n",
    "            print(round(float(count*100)/L),'%')\n",
    "        count=count+1\n",
    "    print('End of prediction')\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_submission(y_pred,X_test_df):\n",
    "\n",
    "    predictions_towrite={}\n",
    "    x_test=X_test_df.values\n",
    "    for i in range(len(y_pred)):\n",
    "        recipients=y_pred[i]\n",
    "        mid=x_test[i][0]\n",
    "        predictions_towrite[mid]=recipients\n",
    "\n",
    "    count=0\n",
    "    with open('./pred_KNN.txt', 'w') as my_file:\n",
    "        my_file.write('mid,recipients' + '\\n')\n",
    "        for ids, preds in predictions_towrite.items():\n",
    "            count=count+1\n",
    "            r=str(ids)+\",\"\n",
    "            for s in preds:\n",
    "                r=r+\" \"+str(s)\n",
    "            r=r+'\\n'\n",
    "            my_file.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_submission(y_pred,X_sub_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
