{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Email recipient recommendation</h1>\n",
    "\n",
    "<i>Thomas Boudou, Guillaume Richard, Antoine Simoulin</i>\n",
    "\n",
    "<p style=\"text-align: justify\">It was shown that at work, employees frequently forget to include one or more recipient(s) before sending a message. Conversely, it is common that some recipients of a given message were actually not intended to receive the message. To increase productivity and prevent information leakage, the needs for effective <b>email recipient recommendation</b> systems are thus pressing.\n",
    "\n",
    "In this challenge, you are asked to develop such a system, which, given the content and the date of a message, recommends a list of <b>10 recipients ranked by decreasing order of relevance</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "%matplotlib inline\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# do not display warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Functions files are saved in \"src/\" directory.\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "from accuracy_measure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "\n",
    "# load files\n",
    "# Data are saved in \"data/\" directory\n",
    "path_to_data = '../mail_recipients/data/'\n",
    "training, training_info, test, test_info, y_df = load_data(path_to_data)\n",
    "\n",
    "# create adress book\n",
    "# /!\\ can take 1-2 min\n",
    "address_books = create_address_books(training, y_df)\n",
    "\n",
    "# join train and test files\n",
    "X_df = join_data(training_info, training)\n",
    "X_sub_df = join_data(test_info, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TF-IDF </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "class TFIDF():\n",
    "    def __init__(self):\n",
    "        self.token_dict = {}\n",
    "        self.tfidf=TfidfVectorizer(tokenizer=None, stop_words='english')\n",
    "\n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[0]):\n",
    "            text = X.body.values[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "            self.token_dict[i] = y\n",
    "\n",
    "        self.tfidf.fit(self.token_dict.values())\n",
    "\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        start_time = time.time()\n",
    "        for i in range(X.shape[0]):\n",
    "            text = X.body.values[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "\n",
    "            self.token_dict[i] = y\n",
    "\n",
    "        X_tfidf = self.tfidf.fit_transform(self.token_dict.values())\n",
    "\n",
    "        print('performed Tf-Idf in %2i seconds.' % (time.time() - start_time))\n",
    "        return X_tfidf\n",
    "\n",
    "    def transform(self, Y):\n",
    "        start_time = time.time()\n",
    "        Y_dict={}\n",
    "        for i in range(Y.shape[0]):\n",
    "            text = Y.body.values[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "\n",
    "            Y_dict[i] = y\n",
    "        Y_tf_idf=self.tfidf.transform(Y_dict.values())\n",
    "\n",
    "        print('performed Tf-Idf in %2i seconds.' % (time.time() - start_time))\n",
    "        return Y_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Useful functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from proper_name_extractor import *\n",
    "\n",
    "#Score vector creation\n",
    "def score_vector(KNN_indices,sender_index,sender_AB,y,cos_dist_mat):\n",
    "    recipient_scores=np.zeros((sum(sender_index),len(sender_AB)+1))\n",
    "    for i in range(sum(sender_index)):\n",
    "        d=np.array(KNN_indices[i])\n",
    "        neigh_mails=y.values[d]#neighbour mails\n",
    "        z=0\n",
    "        for n_mail in neigh_mails:\n",
    "            for rec in n_mail:\n",
    "                if rec in sender_AB:\n",
    "                    j=sender_AB[rec]#index in the score vector\n",
    "                    recipient_scores[i,j]+=cos_dist_mat[i,d[z]]\n",
    "            z=z+1\n",
    "    return recipient_scores\n",
    "\n",
    "#Label creation (from recipient addresses to 0/1 vector)\n",
    "def create_labels(sender_train_is,sender_AB,y_train):\n",
    "    recipient_labels=np.zeros((sum(sender_train_is),len(sender_AB)))\n",
    "    i=0\n",
    "    for rec_list in y_train[sender_train_is]:\n",
    "        for rec in rec_list:\n",
    "            if rec in sender_AB:\n",
    "                j=sender_AB[rec]\n",
    "                recipient_labels[i,j]=1 \n",
    "        i=i+1\n",
    "    return recipient_labels\n",
    "\n",
    "\n",
    "def create_name_scores(X_test,sender_test_is,sender_AB,dict_recipients,dict_names):\n",
    "    recipient_name_score=np.zeros((sum(sender_test_is),len(sender_AB)))\n",
    "    i=0\n",
    "    for names in X_test.ix[sender_test_is].names:\n",
    "        if len(names)>0:\n",
    "            for n in names:\n",
    "                for rec in sender_AB:\n",
    "                    if recipient_surnames[rec]==n:\n",
    "                        recipient_name_score[i,sender_AB[rec]]=1\n",
    "                    \n",
    "    return recipient_name_score\n",
    "#Complete prediction when <10\n",
    "def complete_prediction(k, sender, address_books, res_temp, K=10):\n",
    "    # k the number of recipients to predict\n",
    "    k_most = [elt[0] for elt in address_books[sender][:K] if elt not in res_temp]\n",
    "    k_most = k_most[:k]\n",
    "    if len(k_most) < k: # sender n'a pas assez de contacts\n",
    "        k_most.extend([0] * (k-len(k_most)))\n",
    "    return k_most\n",
    "\n",
    "#Computes the KNN on the distance matrix\n",
    "def KNN(distance,k=30):\n",
    "    indexes=[]\n",
    "    for d in distance:\n",
    "        indexes.append((-d).argsort()[:k])\n",
    "    return np.array(indexes)\n",
    "\n",
    "#Extract names at the beginning of the document\n",
    "def extract_names(text, dict_n=dict_names, dict_m=dict_months,nb_words=5):\n",
    "    \n",
    "    name_list=[]\n",
    "    text=re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    forward=False\n",
    "    \n",
    "    if nb_words==None:\n",
    "        nb_words=len(text.split())\n",
    "    \n",
    "    dear=False\n",
    "    \n",
    "    count=0\n",
    "    for z in text.split()[:nb_words]:\n",
    "        if (z.lower()=='forwarded' or z.lower()=='original'):\n",
    "            forward=True\n",
    "        if(z.lower()=='dear' or z.lower()=='hi' or z.lower()=='thanks'):\n",
    "            dear=True\n",
    "            \n",
    "        if (z.lower() in dict_n and z.lower()!=z and z.lower() not in dict_m and forward==False and (dear==True or count==0)):\n",
    "            name_list.append(z.lower())\n",
    "\n",
    "    if len(name_list)==0:\n",
    "        name_list=['']\n",
    "\n",
    "    return name_list#','.join([word for word in np.unique(name_list).tolist()])\n",
    "\n",
    "#Add columns to the initial dataframe\n",
    "def create_names_df(X_df):\n",
    "    X_names=X_df.copy()\n",
    "\n",
    "    list_names=[]\n",
    "    for x in X_names.body:\n",
    "        l_names=extract_names(x,nb_words=5)\n",
    "        list_names.append(l_names)\n",
    "    X_names['names']=list_names\n",
    "    return X_names\n",
    "\n",
    "#attributes names for mail addresses\n",
    "def names(address_books):\n",
    "    recipient_name = {}\n",
    "    for sender in address_books:\n",
    "        for rec, value in address_books[sender]:\n",
    "            if rec not in recipient_name:\n",
    "                recipient_name[rec]='DefaultNULL'\n",
    "                if '.' in rec[:rec.find('@')]:\n",
    "                    found = rec[:rec.find('.')].lower()\n",
    "                    if found in dict_names:\n",
    "                        recipient_name[rec] = found\n",
    "                    else:\n",
    "                        found=rec[rec.find('.')+1:rec.find('@')].lower()\n",
    "                        if found in dict_names:\n",
    "                            recipient_name[rec] = found\n",
    "    return recipient_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Fitting </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Cross-Validation Module--------\n",
      "\n",
      " Beginning of extraction \n",
      " ------------\n",
      "performed Tf-Idf in 17 seconds.\n",
      "performed Tf-Idf in  4 seconds.\n",
      "Extraction done \n",
      " ------------\n",
      "\n",
      " Beginning of prediction \n",
      " ------------\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "End of prediction\n",
      "------------\n",
      "\n",
      " Beginning of extraction \n",
      " ------------\n",
      "performed Tf-Idf in 17 seconds.\n",
      "performed Tf-Idf in  5 seconds.\n",
      "Extraction done \n",
      " ------------\n",
      "\n",
      " Beginning of prediction \n",
      " ------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-0f314d7b108d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"#import TFIDF_mod\\n#from TFIDF_mod import TFIDF\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import AdaBoostClassifier\\nimport xgboost as xgb\\nimport numpy as np\\nfrom sklearn.model_selection import ShuffleSplit\\nfrom sklearn.model_selection import KFold\\n\\n# splitting data for cross validation\\n#skf = ShuffleSplit(n_splits=1, test_size=0.02)\\nskf=KFold(n_splits=5, shuffle=True)\\nX_df=create_names_df(X_df)\\n\\nprint('--------Cross-Validation Module--------')\\nfor train_is, test_is in skf.split(y_df):\\n    print('\\\\n Beginning of extraction \\\\n ------------')\\n    ############Extraction + TF-IDF############\\n    X_train=X_df.ix[train_is]\\n    y_train = y_df.recipients.loc[train_is].copy()\\n    X_test=X_df.ix[test_is].copy()\\n    y_test = y_df.recipients.loc[test_is].copy()\\n    y_pred=y_test.copy()\\n    \\n    tf_idf = TFIDF()\\n    X_train_TFIDF=tf_idf.fit_transform(X_train)\\n    X_test_TFIDF=tf_idf.transform(X_test)\\n    \\n    print('Extraction done \\\\n ------------')\\n    print('\\\\n Beginning of prediction \\\\n ------------')\\n    ############Prediction############\\n    sender_test = X_test.sender.unique().tolist()\\n    clf={}\\n    count=0\\n    L=len(sender_test)\\n    y_pred=y_test.copy()\\n    \\n    tot_rec_mails={}\\n    for sender in sender_test:\\n        for x in address_books[sender]:\\n            if x[0] in tot_rec_mails:\\n                tot_rec_mails[x[0]]=tot_rec_mails[x[0]]+x[1]\\n            else:\\n                tot_rec_mails[x[0]]=x[1]\\n    for sender in sender_test:\\n        #Isolation of sender's mails\\n        sender_train_is = np.array(X_train.sender == sender)\\n        sender_test_is = np.array(X_test.sender == sender)\\n\\n        ############Feature extraction############\\n        \\n        #Finding the nearest neighbours of sender's mails\\n        cos_dist_mat=cosine_similarity(X_train_TFIDF[sender_train_is])-np.identity(sum(sender_train_is))\\n        cos_dist_mat_test=cosine_similarity(X_test_TFIDF[sender_test_is],X_train_TFIDF[sender_train_is])\\n        #cos_dist_mat=cosine_similarity(X_TFIDF[sender_train_is],X_TFIDF) #to try later\\n        \\n        #KNN\\n        KNN_indices=KNN(cos_dist_mat,k=50)\\n        KNN_indices_test=KNN(cos_dist_mat_test,k=50)\\n\\n        #Sender number in the address book\\n        sender_AB={}\\n        id_to_sender={}\\n        sent_frequency={}\\n        rec_frequency={}\\n        n_mails=float(sum(sender_train_is))\\n        z=0\\n        for x in address_books[sender]:\\n            sender_AB[x[0]]=z\\n            id_to_sender[z]=x[0]\\n            sent_frequency[x[0]]=x[1]/n_mails\\n            rec_frequency[x[0]]=float(x[1])/tot_rec_mails[x[0]]\\n            z=z+1\\n\\n        #Creation of the score vector\\n        recipient_scores=score_vector(KNN_indices,sender_train_is,sender_AB,y_train,cos_dist_mat) \\n\\n        #recipient_name_score=create_name_scores(X_train,sender_train_is,sender_AB,dict_recipients,dict_names)\\n        ############Train############\\n\\n        #Creation of the labels for the classifier\\n        recipient_labels=create_labels(sender_train_is,sender_AB,y_train)\\n\\n        #One classifier per recipient\\n        for rec in sender_AB:\\n            #Adding frequency feature\\n            #recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\\n            x_fit=np.array([[x, sent_frequency[rec], rec_frequency[rec]] for x in recipient_scores.T[sender_AB[rec]]])\\n            #x_fit=np.concatenate((x_fit,\\n            #          recipient_name_score.T[sender_AB[rec]].reshape((len(recipient_name_score.T[sender_AB[rec]]),1))),\\n            #         axis=1)\\n            key=sender+','+rec\\n            #clf[key]=SVC()\\n            clf[key]=xgb.XGBClassifier(n_estimators=10)\\n            clf[key].fit(x_fit,recipient_labels.T[sender_AB[rec]])\\n\\n\\n        ############Test############\\n\\n        #Creation of the test score vector\\n        recipient_scores=score_vector(KNN_indices_test,sender_test_is,sender_AB,y_train,cos_dist_mat_test) \\n        recipient_labels=np.zeros((sum(sender_test_is),len(sender_AB))).T\\n        recipient_name_score=create_name_scores(X_test,sender_test_is,sender_AB,dict_recipients,dict_names)\\n\\n        #Prediction\\n        pred=0\\n        for rec in sender_AB:\\n            #Adding frequency feature\\n            recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\\n            x_fit=np.array([[x, \\n                             sent_frequency[rec], \\n                             rec_frequency[rec]] \\n                            for x in recipient_scores.T[sender_AB[rec]]])\\n            #x_fit=np.concatenate((x_fit,\\n            #          recipient_name_score.T[sender_AB[rec]].reshape((len(recipient_name_score.T[sender_AB[rec]]),1))),\\n            #         axis=1)\\n            \\n            #Predict\\n            key=sender+','+rec\\n            recipient_labels[sender_AB[rec]]=(clf[key].predict_proba(x_fit)).T[1].T\\n            j=0\\n            for names in X_test.ix[sender_test_is].names:\\n                for n in names:\\n                    if recipient_surnames[rec]==n:\\n                        recipient_labels[sender_AB[rec],j]=1\\n                j=j+1\\n        recipient_labels=recipient_labels.T\\n        #Storage\\n        y_test_pred=[]\\n        for y in recipient_labels:\\n            y_tmp=[]\\n            max_rec=(-y).argsort()[:10]\\n            for rec_id in max_rec:\\n                y_tmp.append(id_to_sender[rec_id])\\n            if len(y_tmp) < 10:\\n                y_tmp.extend(complete_prediction(10-len(y_tmp),sender, address_books, y_tmp))\\n            y_test_pred.append(y_tmp)\\n        y_pred.ix[sender_test_is]=y_test_pred\\n        if int((count*10)/L)>int(((count-1)*10)/L):\\n            print(round(float(count*100)/L))\\n        count=count+1\\n    print('End of prediction')\\n    print('------------')\\n\\nfor train_is, test_is in skf.split(y_df):\\n    i=0\\n    accuracy = {}\\n    accuracy_freq = {}\\n    accuracy_TOT = 0\\n    for sender in sender_test:\\n        print('%10s | %40s | ' %(sender_test.index(sender), sender), end='')\\n        sender_train_is = np.array(X_train.sender == sender)\\n        sender_test_is = np.array(X_test.sender == sender)\\n        accuracy[sender] = mapk(y_test[sender_test_is], y_pred[sender_test_is])\\n        accuracy_TOT += accuracy[sender]\\n        print(round(accuracy[sender],2))\\nprint(accuracy_TOT/len(accuracy))\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[0;32m    443\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                               verbose_eval=verbose)\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#import TFIDF_mod\n",
    "#from TFIDF_mod import TFIDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# splitting data for cross validation\n",
    "#skf = ShuffleSplit(n_splits=1, test_size=0.02)\n",
    "skf=KFold(n_splits=5, shuffle=True)\n",
    "X_df=create_names_df(X_df)\n",
    "\n",
    "X_tot=pd.merge(X_df,y_df,on='mid')\n",
    "\n",
    "dict_recipients={}\n",
    "j=0\n",
    "for i,x in X_tot.iterrows():\n",
    "    for rec in x.recipients:\n",
    "        if rec not in dict_recipients:\n",
    "            dict_recipients[rec]=j\n",
    "            j=j+1\n",
    "            \n",
    "recipient_surnames=names(address_books)            \n",
    "print('--------Cross-Validation Module--------')\n",
    "for train_is, test_is in skf.split(y_df):\n",
    "    print('\\n Beginning of extraction \\n ------------')\n",
    "    ############Extraction + TF-IDF############\n",
    "    X_train=X_df.ix[train_is]\n",
    "    y_train = y_df.recipients.loc[train_is].copy()\n",
    "    X_test=X_df.ix[test_is].copy()\n",
    "    y_test = y_df.recipients.loc[test_is].copy()\n",
    "    y_pred=y_test.copy()\n",
    "    \n",
    "    tf_idf = TFIDF()\n",
    "    X_train_TFIDF=tf_idf.fit_transform(X_train)\n",
    "    X_test_TFIDF=tf_idf.transform(X_test)\n",
    "    \n",
    "    print('Extraction done \\n ------------')\n",
    "    print('\\n Beginning of prediction \\n ------------')\n",
    "    ############Prediction############\n",
    "    sender_test = X_test.sender.unique().tolist()\n",
    "    clf={}\n",
    "    count=0\n",
    "    L=len(sender_test)\n",
    "    y_pred=y_test.copy()\n",
    "    for sender in sender_test:\n",
    "        #Isolation of sender's mails\n",
    "        sender_train_is = np.array(X_train.sender == sender)\n",
    "        sender_test_is = np.array(X_test.sender == sender)\n",
    "\n",
    "        ############Feature extraction############\n",
    "        \n",
    "        #Finding the nearest neighbours of sender's mails\n",
    "        cos_dist_mat=cosine_similarity(X_train_TFIDF[sender_train_is])-np.identity(sum(sender_train_is))\n",
    "        cos_dist_mat_test=cosine_similarity(X_test_TFIDF[sender_test_is],X_train_TFIDF[sender_train_is])\n",
    "        #cos_dist_mat=cosine_similarity(X_TFIDF[sender_train_is],X_TFIDF) #to try later\n",
    "        \n",
    "        #KNN\n",
    "        KNN_indices=KNN(cos_dist_mat,k=50)\n",
    "        KNN_indices_test=KNN(cos_dist_mat_test,k=50)\n",
    "\n",
    "        #Sender number in the address book\n",
    "        sender_AB={}\n",
    "        id_to_sender={}\n",
    "        sent_frequency={}\n",
    "        rec_frequency={}\n",
    "        n_mails=float(sum(sender_train_is))\n",
    "        z=0\n",
    "        for x in address_books[sender]:\n",
    "            sender_AB[x[0]]=z\n",
    "            id_to_sender[z]=x[0]\n",
    "            sent_frequency[x[0]]=x[1]/n_mails\n",
    "            rec_frequency[x[0]]=float(x[1])/tot_rec_mails[x[0]]\n",
    "            z=z+1\n",
    "\n",
    "        #Creation of the score vector\n",
    "        recipient_scores=score_vector(KNN_indices,sender_train_is,sender_AB,y_train,cos_dist_mat) \n",
    "\n",
    "        #recipient_name_score=create_name_scores(X_train,sender_train_is,sender_AB,dict_recipients,dict_names)\n",
    "        ############Train############\n",
    "\n",
    "        #Creation of the labels for the classifier\n",
    "        recipient_labels=create_labels(sender_train_is,sender_AB,y_train)\n",
    "\n",
    "        #One classifier per recipient\n",
    "        for rec in sender_AB:\n",
    "            #Adding frequency feature\n",
    "            #recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\n",
    "            x_fit=np.array([[x, sent_frequency[rec], rec_frequency[rec]] for x in recipient_scores.T[sender_AB[rec]]])\n",
    "            #x_fit=np.concatenate((x_fit,\n",
    "            #          recipient_name_score.T[sender_AB[rec]].reshape((len(recipient_name_score.T[sender_AB[rec]]),1))),\n",
    "            #         axis=1)\n",
    "            key=sender+','+rec\n",
    "            #clf[key]=SVC()\n",
    "            clf[key]=xgb.XGBClassifier(n_estimators=10)\n",
    "            clf[key].fit(x_fit,recipient_labels.T[sender_AB[rec]])\n",
    "\n",
    "\n",
    "        ############Test############\n",
    "\n",
    "        #Creation of the test score vector\n",
    "        recipient_scores=score_vector(KNN_indices_test,sender_test_is,sender_AB,y_train,cos_dist_mat_test) \n",
    "        recipient_labels=np.zeros((sum(sender_test_is),len(sender_AB))).T\n",
    "        recipient_name_score=create_name_scores(X_test,sender_test_is,sender_AB,dict_recipients,dict_names)\n",
    "\n",
    "        #Prediction\n",
    "        pred=0\n",
    "        for rec in sender_AB:\n",
    "            #Adding frequency feature\n",
    "            recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\n",
    "            x_fit=np.array([[x, \n",
    "                             sent_frequency[rec], \n",
    "                             rec_frequency[rec]] \n",
    "                            for x in recipient_scores.T[sender_AB[rec]]])\n",
    "            #x_fit=np.concatenate((x_fit,\n",
    "            #          recipient_name_score.T[sender_AB[rec]].reshape((len(recipient_name_score.T[sender_AB[rec]]),1))),\n",
    "            #         axis=1)\n",
    "            \n",
    "            #Predict\n",
    "            key=sender+','+rec\n",
    "            recipient_labels[sender_AB[rec]]=(clf[key].predict_proba(x_fit)).T[1].T\n",
    "            j=0\n",
    "            for names in X_test.ix[sender_test_is].names:\n",
    "                for n in names:\n",
    "                    if recipient_surnames[rec]==n:\n",
    "                        recipient_labels[sender_AB[rec],j]=1\n",
    "                j=j+1\n",
    "        recipient_labels=recipient_labels.T\n",
    "        #Storage\n",
    "        y_test_pred=[]\n",
    "        for y in recipient_labels:\n",
    "            y_tmp=[]\n",
    "            max_rec=(-y).argsort()[:10]\n",
    "            for rec_id in max_rec:\n",
    "                y_tmp.append(id_to_sender[rec_id])\n",
    "            if len(y_tmp) < 10:\n",
    "                y_tmp.extend(complete_prediction(10-len(y_tmp),sender, address_books, y_tmp))\n",
    "            y_test_pred.append(y_tmp)\n",
    "        y_pred.ix[sender_test_is]=y_test_pred\n",
    "        if int((count*10)/L)>int(((count-1)*10)/L):\n",
    "            print(round(float(count*100)/L))\n",
    "        count=count+1\n",
    "    print('End of prediction')\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Submission </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Cross-Validation Module--------\n",
      "\n",
      " Beginning of extraction \n",
      " ------------\n",
      "performed Tf-Idf in 22 seconds.\n",
      "performed Tf-Idf in  1 seconds.\n",
      "Extraction done \n",
      " ------------\n",
      "\n",
      " Beginning of prediction \n",
      " ------------\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "End of prediction\n",
      "------------\n",
      "CPU times: user 8min 57s, sys: 11.5 s, total: 9min 8s\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#import TFIDF_mod\n",
    "#from TFIDF_mod import TFIDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# splitting data for cross validation\n",
    "skf = ShuffleSplit(n_splits=1, test_size=0.02)\n",
    "#skf=KFold(n_splits=5, shuffle=True)\n",
    "X_df=create_names_df(X_df)\n",
    "X_sub_df=create_names_df(X_sub_df)\n",
    "\n",
    "print('--------Cross-Validation Module--------')\n",
    "for train_is, test_is in skf.split(y_df):\n",
    "    print('\\n Beginning of extraction \\n ------------')\n",
    "    ############Extraction + TF-IDF############\n",
    "    X_train=X_df.copy()\n",
    "    y_train = y_df.recipients.copy()\n",
    "    X_test=X_sub_df.copy()\n",
    "    #y_test = y_df.recipients.loc[test_is].copy()\n",
    "    y_pred=pd.Series([[''] for x in X_sub_df.sender])\n",
    "\n",
    "    \n",
    "    tf_idf = TFIDF()\n",
    "    X_train_TFIDF=tf_idf.fit_transform(X_train)\n",
    "    X_test_TFIDF=tf_idf.transform(X_test)\n",
    "    \n",
    "    print('Extraction done \\n ------------')\n",
    "    print('\\n Beginning of prediction \\n ------------')\n",
    "    ############Prediction############\n",
    "    sender_test = X_test.sender.unique().tolist()\n",
    "    clf={}\n",
    "    count=0\n",
    "    L=len(sender_test)\n",
    "    \n",
    "    tot_rec_mails={}\n",
    "    for sender in sender_test:\n",
    "        for x in address_books[sender]:\n",
    "            if x[0] in tot_rec_mails:\n",
    "                tot_rec_mails[x[0]]=tot_rec_mails[x[0]]+x[1]\n",
    "            else:\n",
    "                tot_rec_mails[x[0]]=x[1]\n",
    "    for sender in sender_test:\n",
    "        #Isolation of sender's mails\n",
    "        sender_train_is = np.array(X_train.sender == sender)\n",
    "        sender_test_is = np.array(X_test.sender == sender)\n",
    "\n",
    "        ############Feature extraction############\n",
    "        \n",
    "        #Finding the nearest neighbours of sender's mails\n",
    "        cos_dist_mat=cosine_similarity(X_train_TFIDF[sender_train_is])-np.identity(sum(sender_train_is))\n",
    "        cos_dist_mat_test=cosine_similarity(X_test_TFIDF[sender_test_is],X_train_TFIDF[sender_train_is])\n",
    "        #cos_dist_mat=cosine_similarity(X_TFIDF[sender_train_is],X_TFIDF) #to try later\n",
    "        \n",
    "        #KNN\n",
    "        KNN_indices=KNN(cos_dist_mat,k=50)\n",
    "        KNN_indices_test=KNN(cos_dist_mat_test,k=50)\n",
    "\n",
    "        #Sender number in the address book\n",
    "        sender_AB={}\n",
    "        id_to_sender={}\n",
    "        sent_frequency={}\n",
    "        rec_frequency={}\n",
    "        n_mails=float(sum(sender_train_is))\n",
    "        z=0\n",
    "        for x in address_books[sender]:\n",
    "            sender_AB[x[0]]=z\n",
    "            id_to_sender[z]=x[0]\n",
    "            sent_frequency[x[0]]=x[1]/n_mails\n",
    "            rec_frequency[x[0]]=float(x[1])/tot_rec_mails[x[0]]\n",
    "            z=z+1\n",
    "\n",
    "        #Creation of the score vector\n",
    "        recipient_scores=score_vector(KNN_indices,sender_train_is,sender_AB,y_train,cos_dist_mat) \n",
    "\n",
    "        #recipient_name_score=create_name_scores(X_train,sender_train_is,sender_AB,dict_recipients,dict_names)\n",
    "        ############Train############\n",
    "\n",
    "        #Creation of the labels for the classifier\n",
    "        recipient_labels=create_labels(sender_train_is,sender_AB,y_train)\n",
    "\n",
    "        #One classifier per recipient\n",
    "        for rec in sender_AB:\n",
    "            #Adding frequency feature\n",
    "            #recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\n",
    "            x_fit=np.array([[x, sent_frequency[rec], rec_frequency[rec]] for x in recipient_scores.T[sender_AB[rec]]])\n",
    "            #x_fit=np.concatenate((x_fit,\n",
    "            #          recipient_name_score.T[sender_AB[rec]].reshape((len(recipient_name_score.T[sender_AB[rec]]),1))),\n",
    "            #         axis=1)\n",
    "            key=sender+','+rec\n",
    "            #clf[key]=SVC()\n",
    "            clf[key]=xgb.XGBClassifier(n_estimators=10)\n",
    "            clf[key].fit(x_fit,recipient_labels.T[sender_AB[rec]])\n",
    "\n",
    "\n",
    "        ############Test############\n",
    "\n",
    "        #Creation of the test score vector\n",
    "        recipient_scores=score_vector(KNN_indices_test,sender_test_is,sender_AB,y_train,cos_dist_mat_test) \n",
    "        recipient_labels=np.zeros((sum(sender_test_is),len(sender_AB))).T\n",
    "        recipient_name_score=create_name_scores(X_test,sender_test_is,sender_AB,dict_recipients,dict_names)\n",
    "\n",
    "        #Prediction\n",
    "        pred=0\n",
    "        for rec in sender_AB:\n",
    "            #Adding frequency feature\n",
    "            recipient_scores.T[len(sender_AB)]=sent_frequency[rec]\n",
    "            x_fit=np.array([[x, \n",
    "                             sent_frequency[rec], \n",
    "                             rec_frequency[rec]] \n",
    "                            for x in recipient_scores.T[sender_AB[rec]]])\n",
    "            #x_fit=np.concatenate((x_fit,\n",
    "            #          recipient_name_score.T[sender_AB[rec]].reshape((len(recipient_name_score.T[sender_AB[rec]]),1))),\n",
    "            #         axis=1)\n",
    "            \n",
    "            #Predict\n",
    "            key=sender+','+rec\n",
    "            recipient_labels[sender_AB[rec]]=(clf[key].predict_proba(x_fit)).T[1].T\n",
    "            j=0\n",
    "            for names in X_test.ix[sender_test_is].names:\n",
    "                for n in names:\n",
    "                    if recipient_surnames[rec]==n:\n",
    "                        recipient_labels[sender_AB[rec],j]=1\n",
    "                j=j+1\n",
    "        recipient_labels=recipient_labels.T\n",
    "        #Storage\n",
    "        y_test_pred=[]\n",
    "        for y in recipient_labels:\n",
    "            y_tmp=[]\n",
    "            max_rec=(-y).argsort()[:10]\n",
    "            for rec_id in max_rec:\n",
    "                y_tmp.append(id_to_sender[rec_id])\n",
    "            if len(y_tmp) < 10:\n",
    "                y_tmp.extend(complete_prediction(10-len(y_tmp),sender, address_books, y_tmp))\n",
    "            y_test_pred.append(y_tmp)\n",
    "        y_pred.ix[sender_test_is]=y_test_pred\n",
    "        if int((count*10)/L)>int(((count-1)*10)/L):\n",
    "            print(round(float(count*100)/L))\n",
    "        count=count+1\n",
    "    print('End of prediction')\n",
    "    print('------------')\n",
    "\n",
    "create_submission(y_pred,X_sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_submission(y_pred,X_test_df):\n",
    "\n",
    "    predictions_towrite={}\n",
    "    x_test=X_test_df.values\n",
    "    for i in range(len(y_pred)):\n",
    "        recipients=y_pred[i]\n",
    "        mid=x_test[i][0]\n",
    "        predictions_towrite[mid]=recipients\n",
    "\n",
    "    count=0\n",
    "    with open('./pred_custom.txt', 'w') as my_file:\n",
    "        my_file.write('mid,recipients' + '\\n')\n",
    "        for ids, preds in predictions_towrite.items():\n",
    "            count=count+1\n",
    "            r=str(ids)+\",\"\n",
    "            for s in preds:\n",
    "                r=r+\" \"+str(s)\n",
    "            r=r+'\\n'\n",
    "            my_file.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_submission(y_pred,X_sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>sender</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1577</td>\n",
       "      <td>2001-11-19 06:59:51</td>\n",
       "      <td>Note:  Stocks of heating oil are very high for...</td>\n",
       "      <td>lorna.brennan@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1750</td>\n",
       "      <td>2002-03-05 08:46:57</td>\n",
       "      <td>Kevin Hyatt and I are going for \"sghetti\" at S...</td>\n",
       "      <td>julie.armstrong@enron.com</td>\n",
       "      <td>[kevin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1916</td>\n",
       "      <td>2002-02-13 14:17:39</td>\n",
       "      <td>This was forwarded to me and it is funny. - Wi...</td>\n",
       "      <td>julie.armstrong@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2094</td>\n",
       "      <td>2002-01-22 11:33:56</td>\n",
       "      <td>I will be in to and happy to assist too.  I ma...</td>\n",
       "      <td>julie.armstrong@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2205</td>\n",
       "      <td>2002-01-11 07:12:19</td>\n",
       "      <td>Thanks. I needed a morning chuckle.</td>\n",
       "      <td>julie.armstrong@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2297</td>\n",
       "      <td>2002-01-11 14:37:19</td>\n",
       "      <td>Note:  Westpath Expansion plans filed at NEBTr...</td>\n",
       "      <td>lorna.brennan@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5300</td>\n",
       "      <td>2001-11-26 14:13:01</td>\n",
       "      <td>Here are Peggy s slides. -----Original Message...</td>\n",
       "      <td>stanley.horton@enron.com</td>\n",
       "      <td>[peggy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5333</td>\n",
       "      <td>2001-11-19 13:44:18</td>\n",
       "      <td>Here s the information. -----Original Message-...</td>\n",
       "      <td>cindy.stark@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6583</td>\n",
       "      <td>2002-01-18 05:00:48</td>\n",
       "      <td>I would like to know where and how this is goi...</td>\n",
       "      <td>darrell.schoolcraft@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7460</td>\n",
       "      <td>2001-11-12 16:43:31</td>\n",
       "      <td>Richard: Per Elliot s e-mail below, do you hav...</td>\n",
       "      <td>jennifer.thome@enron.com</td>\n",
       "      <td>[richard, elliot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7491</td>\n",
       "      <td>2001-11-05 09:56:44</td>\n",
       "      <td>Note also that NARUC s annual conference is in...</td>\n",
       "      <td>jennifer.thome@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18171</td>\n",
       "      <td>2001-11-15 06:40:33</td>\n",
       "      <td>Brent is moving a gas line in his house this m...</td>\n",
       "      <td>becky.spencer@enron.com</td>\n",
       "      <td>[brent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18173</td>\n",
       "      <td>2001-11-14 08:24:49</td>\n",
       "      <td>Good Morning!I just received a call from Chery...</td>\n",
       "      <td>keegan.farrell@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18363</td>\n",
       "      <td>2001-12-15 11:14:12</td>\n",
       "      <td>QE2</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18847</td>\n",
       "      <td>2001-11-16 07:34:52</td>\n",
       "      <td>ATTORNEY CLIENT COMMUNICATIONATTORNEY WORK PRO...</td>\n",
       "      <td>mark.greenberg@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18851</td>\n",
       "      <td>2001-11-15 10:35:01</td>\n",
       "      <td>Rick Hammett has confirmed that there were no ...</td>\n",
       "      <td>c..williams@enron.com</td>\n",
       "      <td>[rick]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18854</td>\n",
       "      <td>2001-11-15 09:24:06</td>\n",
       "      <td>To All -Below is a lined version of the ICE co...</td>\n",
       "      <td>mark.greenberg@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18866</td>\n",
       "      <td>2001-11-14 12:24:02</td>\n",
       "      <td>Brent -FYIMark -----Original Message-----From:...</td>\n",
       "      <td>mark.greenberg@enron.com</td>\n",
       "      <td>[brent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18870</td>\n",
       "      <td>2001-11-12 13:15:06</td>\n",
       "      <td>Mark\\tAll work on the application for a power ...</td>\n",
       "      <td>marcus.nettelton@enron.com</td>\n",
       "      <td>[mark]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19218</td>\n",
       "      <td>2001-11-27 17:20:20</td>\n",
       "      <td>I think there is a real possibility here.  It ...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19220</td>\n",
       "      <td>2001-11-27 11:37:26</td>\n",
       "      <td>There have been some ongoing disputes with AEP...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19222</td>\n",
       "      <td>2001-11-27 07:36:15</td>\n",
       "      <td>Yet another idea:A flashlight that has a magne...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19228</td>\n",
       "      <td>2001-11-20 15:55:17</td>\n",
       "      <td>No gifts, thank you -- just a social hour.</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19237</td>\n",
       "      <td>2001-11-14 08:22:32</td>\n",
       "      <td>Sorry about that.  I have been a bit preoccupi...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19245</td>\n",
       "      <td>2001-11-09 10:02:50</td>\n",
       "      <td>Net Works is not a marketing affiliate.  We ca...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19250</td>\n",
       "      <td>2001-11-09 07:10:46</td>\n",
       "      <td>Thanks for the offer.  At this moment, looks l...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19254</td>\n",
       "      <td>2001-11-08 11:55:18</td>\n",
       "      <td>Thanks David -- I appreciate that.  I m thinki...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[david]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19264</td>\n",
       "      <td>2001-11-06 11:56:22</td>\n",
       "      <td>You can confirm that it is not regulated and s...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19281</td>\n",
       "      <td>2001-11-05 07:57:11</td>\n",
       "      <td>-----Original Message-----From: \\tTaylor, Mar...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19286</td>\n",
       "      <td>2001-11-02 11:44:43</td>\n",
       "      <td>-----Original Message-----From: \\tVan Hooser,...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>394431</td>\n",
       "      <td>2001-11-29 13:58:51</td>\n",
       "      <td>The following expense report is ready for appr...</td>\n",
       "      <td>enron_update@concureworkplace.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>397831</td>\n",
       "      <td>2002-01-07 16:09:43</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTHi Kam,Per y...</td>\n",
       "      <td>cheryl.johnson@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>397835</td>\n",
       "      <td>2002-01-08 09:08:00</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTHi Kam,The a...</td>\n",
       "      <td>cheryl.johnson@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>397887</td>\n",
       "      <td>2002-01-14 07:03:15</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTIt looks lik...</td>\n",
       "      <td>shona.wilson@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>397982</td>\n",
       "      <td>2002-01-24 08:00:39</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTWhen: Thursd...</td>\n",
       "      <td>shona.wilson@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>398080</td>\n",
       "      <td>2002-01-30 11:22:14</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTLooks like w...</td>\n",
       "      <td>phillip.m.love@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>398120</td>\n",
       "      <td>2002-01-31 13:59:35</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTHere is an u...</td>\n",
       "      <td>shona.wilson@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>398129</td>\n",
       "      <td>2002-02-01 09:49:18</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTHere is the ...</td>\n",
       "      <td>shona.wilson@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>398136</td>\n",
       "      <td>2002-02-01 12:00:41</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTPlease plan ...</td>\n",
       "      <td>christina.valdez@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>398182</td>\n",
       "      <td>2002-02-05 15:03:00</td>\n",
       "      <td>X-FileName: kam keiser 7-11-02.PSTHere is the ...</td>\n",
       "      <td>shona.wilson@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>398254</td>\n",
       "      <td>2001-11-02 07:57:50</td>\n",
       "      <td>------------From:\\t\"Gina Pelham\" &lt;g.pelham@wor...</td>\n",
       "      <td>joannie.williamson@enron.com</td>\n",
       "      <td>[gina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>401709</td>\n",
       "      <td>2002-01-14 21:09:40</td>\n",
       "      <td>X-FileName: RTO West is working on a March 1, ...</td>\n",
       "      <td>alan.comnes@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>401854</td>\n",
       "      <td>2001-11-07 19:30:54</td>\n",
       "      <td>X-FileName: I would like to get it to someone ...</td>\n",
       "      <td>alan.comnes@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>401900</td>\n",
       "      <td>2001-12-13 15:15:07</td>\n",
       "      <td>X-FileName: I am told that ISO disbursed to pa...</td>\n",
       "      <td>alan.comnes@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>402047</td>\n",
       "      <td>2002-01-10 19:40:56</td>\n",
       "      <td>X-FileName: Hi Ryan,Julie filled me in on your...</td>\n",
       "      <td>grace.rodriguez@enron.com</td>\n",
       "      <td>[ryan, julie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>402061</td>\n",
       "      <td>2002-02-06 14:51:43</td>\n",
       "      <td>X-FileName: IMPORTANT - THE IDS BELOW WILL BE ...</td>\n",
       "      <td>stephanie.sever@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>402135</td>\n",
       "      <td>2001-11-19 13:49:51</td>\n",
       "      <td>X-FileName: We currently have $21.4 million in...</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>402233</td>\n",
       "      <td>2001-11-08 19:13:45</td>\n",
       "      <td>X-FileName:  Dynegy-Enron Deal Faces Significa...</td>\n",
       "      <td>alan.comnes@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>402288</td>\n",
       "      <td>2001-12-13 13:16:02</td>\n",
       "      <td>X-FileName: In addition to the summary below, ...</td>\n",
       "      <td>alan.comnes@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>402317</td>\n",
       "      <td>2002-01-03 20:32:04</td>\n",
       "      <td>X-FileName: Funny, but it won t get picked up ...</td>\n",
       "      <td>alan.comnes@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>402393</td>\n",
       "      <td>2002-02-05 20:52:05</td>\n",
       "      <td>X-FileName: The new meeting Louise Kitchen ref...</td>\n",
       "      <td>liz.taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>402417</td>\n",
       "      <td>2002-01-14 18:51:06</td>\n",
       "      <td>X-FileName: Attached is the Termination List f...</td>\n",
       "      <td>stephanie.panus@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>402419</td>\n",
       "      <td>2002-01-16 13:23:45</td>\n",
       "      <td>X-FileName: fyi from the bankruptcy attorney.-...</td>\n",
       "      <td>l..nicolay@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>402427</td>\n",
       "      <td>2002-01-18 18:40:17</td>\n",
       "      <td>X-FileName: Attached is the Daily Termination ...</td>\n",
       "      <td>stephanie.panus@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>402848</td>\n",
       "      <td>2002-01-04 18:49:03</td>\n",
       "      <td>X-FileName: The following name overlays were c...</td>\n",
       "      <td>l..denton@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>402855</td>\n",
       "      <td>2001-12-17 15:26:18</td>\n",
       "      <td>X-FileName: My old cell phone has been retired...</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>[my]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>402856</td>\n",
       "      <td>2001-12-19 20:58:18</td>\n",
       "      <td>X-FileName: Confidential:  For Internal Distri...</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>402857</td>\n",
       "      <td>2001-12-28 17:03:22</td>\n",
       "      <td>X-FileName: Please plan on attending a meeting...</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>404890</td>\n",
       "      <td>2001-11-12 11:30:49</td>\n",
       "      <td>X-FileName: Chris Calger and I will be holding...</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>[chris]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>404929</td>\n",
       "      <td>2002-02-06 20:09:37</td>\n",
       "      <td>X-FileName:      There will be two presentatio...</td>\n",
       "      <td>liz.taylor@enron.com</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2362 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mid                 date  \\\n",
       "0       1577  2001-11-19 06:59:51   \n",
       "1       1750  2002-03-05 08:46:57   \n",
       "2       1916  2002-02-13 14:17:39   \n",
       "3       2094  2002-01-22 11:33:56   \n",
       "4       2205  2002-01-11 07:12:19   \n",
       "5       2297  2002-01-11 14:37:19   \n",
       "6       5300  2001-11-26 14:13:01   \n",
       "7       5333  2001-11-19 13:44:18   \n",
       "8       6583  2002-01-18 05:00:48   \n",
       "9       7460  2001-11-12 16:43:31   \n",
       "10      7491  2001-11-05 09:56:44   \n",
       "11     18171  2001-11-15 06:40:33   \n",
       "12     18173  2001-11-14 08:24:49   \n",
       "13     18363  2001-12-15 11:14:12   \n",
       "14     18847  2001-11-16 07:34:52   \n",
       "15     18851  2001-11-15 10:35:01   \n",
       "16     18854  2001-11-15 09:24:06   \n",
       "17     18866  2001-11-14 12:24:02   \n",
       "18     18870  2001-11-12 13:15:06   \n",
       "19     19218  2001-11-27 17:20:20   \n",
       "20     19220  2001-11-27 11:37:26   \n",
       "21     19222  2001-11-27 07:36:15   \n",
       "22     19228  2001-11-20 15:55:17   \n",
       "23     19237  2001-11-14 08:22:32   \n",
       "24     19245  2001-11-09 10:02:50   \n",
       "25     19250  2001-11-09 07:10:46   \n",
       "26     19254  2001-11-08 11:55:18   \n",
       "27     19264  2001-11-06 11:56:22   \n",
       "28     19281  2001-11-05 07:57:11   \n",
       "29     19286  2001-11-02 11:44:43   \n",
       "...      ...                  ...   \n",
       "2332  394431  2001-11-29 13:58:51   \n",
       "2333  397831  2002-01-07 16:09:43   \n",
       "2334  397835  2002-01-08 09:08:00   \n",
       "2335  397887  2002-01-14 07:03:15   \n",
       "2336  397982  2002-01-24 08:00:39   \n",
       "2337  398080  2002-01-30 11:22:14   \n",
       "2338  398120  2002-01-31 13:59:35   \n",
       "2339  398129  2002-02-01 09:49:18   \n",
       "2340  398136  2002-02-01 12:00:41   \n",
       "2341  398182  2002-02-05 15:03:00   \n",
       "2342  398254  2001-11-02 07:57:50   \n",
       "2343  401709  2002-01-14 21:09:40   \n",
       "2344  401854  2001-11-07 19:30:54   \n",
       "2345  401900  2001-12-13 15:15:07   \n",
       "2346  402047  2002-01-10 19:40:56   \n",
       "2347  402061  2002-02-06 14:51:43   \n",
       "2348  402135  2001-11-19 13:49:51   \n",
       "2349  402233  2001-11-08 19:13:45   \n",
       "2350  402288  2001-12-13 13:16:02   \n",
       "2351  402317  2002-01-03 20:32:04   \n",
       "2352  402393  2002-02-05 20:52:05   \n",
       "2353  402417  2002-01-14 18:51:06   \n",
       "2354  402419  2002-01-16 13:23:45   \n",
       "2355  402427  2002-01-18 18:40:17   \n",
       "2356  402848  2002-01-04 18:49:03   \n",
       "2357  402855  2001-12-17 15:26:18   \n",
       "2358  402856  2001-12-19 20:58:18   \n",
       "2359  402857  2001-12-28 17:03:22   \n",
       "2360  404890  2001-11-12 11:30:49   \n",
       "2361  404929  2002-02-06 20:09:37   \n",
       "\n",
       "                                                   body  \\\n",
       "0     Note:  Stocks of heating oil are very high for...   \n",
       "1     Kevin Hyatt and I are going for \"sghetti\" at S...   \n",
       "2     This was forwarded to me and it is funny. - Wi...   \n",
       "3     I will be in to and happy to assist too.  I ma...   \n",
       "4                   Thanks. I needed a morning chuckle.   \n",
       "5     Note:  Westpath Expansion plans filed at NEBTr...   \n",
       "6     Here are Peggy s slides. -----Original Message...   \n",
       "7     Here s the information. -----Original Message-...   \n",
       "8     I would like to know where and how this is goi...   \n",
       "9     Richard: Per Elliot s e-mail below, do you hav...   \n",
       "10    Note also that NARUC s annual conference is in...   \n",
       "11    Brent is moving a gas line in his house this m...   \n",
       "12    Good Morning!I just received a call from Chery...   \n",
       "13                                                  QE2   \n",
       "14    ATTORNEY CLIENT COMMUNICATIONATTORNEY WORK PRO...   \n",
       "15    Rick Hammett has confirmed that there were no ...   \n",
       "16    To All -Below is a lined version of the ICE co...   \n",
       "17    Brent -FYIMark -----Original Message-----From:...   \n",
       "18    Mark\\tAll work on the application for a power ...   \n",
       "19    I think there is a real possibility here.  It ...   \n",
       "20    There have been some ongoing disputes with AEP...   \n",
       "21    Yet another idea:A flashlight that has a magne...   \n",
       "22          No gifts, thank you -- just a social hour.    \n",
       "23    Sorry about that.  I have been a bit preoccupi...   \n",
       "24    Net Works is not a marketing affiliate.  We ca...   \n",
       "25    Thanks for the offer.  At this moment, looks l...   \n",
       "26    Thanks David -- I appreciate that.  I m thinki...   \n",
       "27    You can confirm that it is not regulated and s...   \n",
       "28     -----Original Message-----From: \\tTaylor, Mar...   \n",
       "29     -----Original Message-----From: \\tVan Hooser,...   \n",
       "...                                                 ...   \n",
       "2332  The following expense report is ready for appr...   \n",
       "2333  X-FileName: kam keiser 7-11-02.PSTHi Kam,Per y...   \n",
       "2334  X-FileName: kam keiser 7-11-02.PSTHi Kam,The a...   \n",
       "2335  X-FileName: kam keiser 7-11-02.PSTIt looks lik...   \n",
       "2336  X-FileName: kam keiser 7-11-02.PSTWhen: Thursd...   \n",
       "2337  X-FileName: kam keiser 7-11-02.PSTLooks like w...   \n",
       "2338  X-FileName: kam keiser 7-11-02.PSTHere is an u...   \n",
       "2339  X-FileName: kam keiser 7-11-02.PSTHere is the ...   \n",
       "2340  X-FileName: kam keiser 7-11-02.PSTPlease plan ...   \n",
       "2341  X-FileName: kam keiser 7-11-02.PSTHere is the ...   \n",
       "2342  ------------From:\\t\"Gina Pelham\" <g.pelham@wor...   \n",
       "2343  X-FileName: RTO West is working on a March 1, ...   \n",
       "2344  X-FileName: I would like to get it to someone ...   \n",
       "2345  X-FileName: I am told that ISO disbursed to pa...   \n",
       "2346  X-FileName: Hi Ryan,Julie filled me in on your...   \n",
       "2347  X-FileName: IMPORTANT - THE IDS BELOW WILL BE ...   \n",
       "2348  X-FileName: We currently have $21.4 million in...   \n",
       "2349  X-FileName:  Dynegy-Enron Deal Faces Significa...   \n",
       "2350  X-FileName: In addition to the summary below, ...   \n",
       "2351  X-FileName: Funny, but it won t get picked up ...   \n",
       "2352  X-FileName: The new meeting Louise Kitchen ref...   \n",
       "2353  X-FileName: Attached is the Termination List f...   \n",
       "2354  X-FileName: fyi from the bankruptcy attorney.-...   \n",
       "2355  X-FileName: Attached is the Daily Termination ...   \n",
       "2356  X-FileName: The following name overlays were c...   \n",
       "2357  X-FileName: My old cell phone has been retired...   \n",
       "2358  X-FileName: Confidential:  For Internal Distri...   \n",
       "2359  X-FileName: Please plan on attending a meeting...   \n",
       "2360  X-FileName: Chris Calger and I will be holding...   \n",
       "2361  X-FileName:      There will be two presentatio...   \n",
       "\n",
       "                                 sender              names  \n",
       "0               lorna.brennan@enron.com                 []  \n",
       "1             julie.armstrong@enron.com            [kevin]  \n",
       "2             julie.armstrong@enron.com                 []  \n",
       "3             julie.armstrong@enron.com                 []  \n",
       "4             julie.armstrong@enron.com                 []  \n",
       "5               lorna.brennan@enron.com                 []  \n",
       "6              stanley.horton@enron.com            [peggy]  \n",
       "7                 cindy.stark@enron.com                 []  \n",
       "8         darrell.schoolcraft@enron.com                 []  \n",
       "9              jennifer.thome@enron.com  [richard, elliot]  \n",
       "10             jennifer.thome@enron.com                 []  \n",
       "11              becky.spencer@enron.com            [brent]  \n",
       "12             keegan.farrell@enron.com                 []  \n",
       "13                     taylor@enron.com                 []  \n",
       "14             mark.greenberg@enron.com                 []  \n",
       "15                c..williams@enron.com             [rick]  \n",
       "16             mark.greenberg@enron.com                 []  \n",
       "17             mark.greenberg@enron.com            [brent]  \n",
       "18           marcus.nettelton@enron.com             [mark]  \n",
       "19                     taylor@enron.com                 []  \n",
       "20                     taylor@enron.com                 []  \n",
       "21                     taylor@enron.com                 []  \n",
       "22                     taylor@enron.com                 []  \n",
       "23                     taylor@enron.com                 []  \n",
       "24                     taylor@enron.com                 []  \n",
       "25                     taylor@enron.com                 []  \n",
       "26                     taylor@enron.com            [david]  \n",
       "27                     taylor@enron.com                 []  \n",
       "28                     taylor@enron.com                 []  \n",
       "29                     taylor@enron.com                 []  \n",
       "...                                 ...                ...  \n",
       "2332  enron_update@concureworkplace.com                 []  \n",
       "2333           cheryl.johnson@enron.com                 []  \n",
       "2334           cheryl.johnson@enron.com                 []  \n",
       "2335             shona.wilson@enron.com                 []  \n",
       "2336             shona.wilson@enron.com                 []  \n",
       "2337           phillip.m.love@enron.com                 []  \n",
       "2338             shona.wilson@enron.com                 []  \n",
       "2339             shona.wilson@enron.com                 []  \n",
       "2340         christina.valdez@enron.com                 []  \n",
       "2341             shona.wilson@enron.com                 []  \n",
       "2342       joannie.williamson@enron.com             [gina]  \n",
       "2343              alan.comnes@enron.com                 []  \n",
       "2344              alan.comnes@enron.com                 []  \n",
       "2345              alan.comnes@enron.com                 []  \n",
       "2346          grace.rodriguez@enron.com      [ryan, julie]  \n",
       "2347          stephanie.sever@enron.com                 []  \n",
       "2348               tim.belden@enron.com                 []  \n",
       "2349              alan.comnes@enron.com                 []  \n",
       "2350              alan.comnes@enron.com                 []  \n",
       "2351              alan.comnes@enron.com                 []  \n",
       "2352               liz.taylor@enron.com                 []  \n",
       "2353          stephanie.panus@enron.com                 []  \n",
       "2354               l..nicolay@enron.com                 []  \n",
       "2355          stephanie.panus@enron.com                 []  \n",
       "2356                l..denton@enron.com                 []  \n",
       "2357               tim.belden@enron.com               [my]  \n",
       "2358               tim.belden@enron.com                 []  \n",
       "2359               tim.belden@enron.com                 []  \n",
       "2360               tim.belden@enron.com            [chris]  \n",
       "2361               liz.taylor@enron.com                 []  \n",
       "\n",
       "[2362 rows x 5 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-4a999a2608f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_name_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msender_test_is\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msender_AB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict_recipients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-fe4007607f61>\u001b[0m in \u001b[0;36mcreate_name_scores\u001b[1;34m(X_test, sender_test_is, sender_AB, dict_recipients, dict_names)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mrec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msender_AB\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mdict_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                         \u001b[0mrecipient_name_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msender_AB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "create_name_scores(X_test,sender_test_is,sender_AB,dict_recipients,dict_names)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
