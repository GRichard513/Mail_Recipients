{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Email recipient recommendation</h1>\n",
    "\n",
    "<i>Thomas Boudou, Guillaume Richard, Antoine Simoulin</i>\n",
    "\n",
    "<p style=\"text-align: justify\">It was shown that at work, employees frequently forget to include one or more recipient(s) before sending a message. Conversely, it is common that some recipients of a given message were actually not intended to receive the message. To increase productivity and prevent information leakage, the needs for effective <b>email recipient recommendation</b> systems are thus pressing.\n",
    "\n",
    "In this challenge, you are asked to develop such a system, which, given the content and the date of a message, recommends a list of <b>10 recipients ranked by decreasing order of relevance</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "%matplotlib inline\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# do not display warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Functions files are saved in \"src/\" directory.\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "from accuracy_measure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "\n",
    "# load files\n",
    "# Data are saved in \"data/\" directory\n",
    "path_to_data = 'data/'\n",
    "training, training_info, test, test_info, y_df = load_data(path_to_data)\n",
    "\n",
    "\n",
    "# join train and test files\n",
    "X_df = join_data(training_info, training)\n",
    "X_sub_df = join_data(test_info, test)\n",
    "\n",
    "# remove non authorise adress from y_df\n",
    "y_df = clean(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "class TFIDF():\n",
    "    def __init__(self):\n",
    "        self.token_dict = {}\n",
    "        self.tfidf=TfidfVectorizer(tokenizer=None, stop_words='english')\n",
    "\n",
    "    def fit(self, X):\n",
    "        for i in range(X.shape[0]):\n",
    "            text = X.body[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "            self.token_dict[i] = y\n",
    "\n",
    "        self.tfidf.fit(self.token_dict.values())\n",
    "\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        start_time = time.time()\n",
    "        for i in range(X.shape[0]):\n",
    "            text = X.body[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "            self.token_dict[i] = y\n",
    "\n",
    "        X_tfidf = self.tfidf.fit_transform(self.token_dict.values())\n",
    "\n",
    "        print('performed Tf-Idf in %2i seconds.' % (time.time() - start_time))\n",
    "        return X_tfidf\n",
    "\n",
    "    def transform(self, Y):\n",
    "        start_time = time.time()\n",
    "        Y_dict={}\n",
    "        for i in range(Y.shape[0]):\n",
    "            text = Y.body[i]\n",
    "            lowers = text.lower()\n",
    "            s=string.punctuation.replace('@','')\n",
    "            s=s.replace('+','')\n",
    "\n",
    "            no_punctuation = lowers.translate(str.maketrans('','',s))\n",
    "            y = \" \".join(no_punctuation.split())\n",
    "            y = ' '.join([word for word in y.split() if word not in cachedStopWords])\n",
    "\n",
    "            Y_dict[i] = y\n",
    "        Y_tf_idf=self.tfidf.transform(Y_dict.values())\n",
    "\n",
    "        print('performed Tf-Idf in %2i seconds.' % (time.time() - start_time))\n",
    "        return Y_tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-19ed6bf468bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# /!\\ function can take 1-2 min to execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_TFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFIDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# resulting shape : (43613, 275988) / without tokenizer: (43613, 294517)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_sub_TFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFIDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sub_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0ba7e470f13f>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mX_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'performed Tf-Idf in %2i seconds.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Antoine/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \"\"\"\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Antoine/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0mn_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Antoine/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0mmap_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# transform each mail body into tfidf vector\n",
    "# /!\\ function can take 1-2 min to execute\n",
    "TFIDF = TFIDF()\n",
    "X_TFIDF = TFIDF.fit_transform(X_df) # resulting shape : (43613, 275988) / without tokenizer: (43613, 294517)\n",
    "X_sub_TFIDF = TFIDF.transform(X_sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43613, 294517)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TFIDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predictors</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def complete_prediction(k, sender, address_books, res_temp, K=10):\n",
    "    # k the number of recipients to predict\n",
    "    k_most = [elt[0] for elt in address_books[sender][:K] if elt[0] not in res_temp]\n",
    "    k_most = k_most[:k]\n",
    "    if len(k_most) < k: # sender n'a pas assez de contacts\n",
    "        k_most.extend([0] * (k-len(k_most)))\n",
    "    return k_most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predictor closest message with Tf-Idf</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Predictor_TFIDF():\n",
    "    def __init__(self, X, y, sender, address_books,N=10):\n",
    "        self.train = X\n",
    "        self.predict = y.values\n",
    "        self.sender = sender\n",
    "        self.address_books = address_books\n",
    "        self.N = min(N,10)\n",
    "        \n",
    "    def prediction(self, X):\n",
    "        res = []\n",
    "        for i in range(X.shape[0]):\n",
    "            cos = cosine_similarity(X[i],self.train).argsort()[:,0][0] # mail le plus proche\n",
    "            if self.N != 0:\n",
    "                res_temp = [self.predict[cos][0][:self.N]] # add the N first recipients of the closest e-mail\n",
    "            else:\n",
    "                 res_temp = []\n",
    "            # if less than 10 recipients, complete the prediction with more frequents users\n",
    "            if len(res_temp) < 10:\n",
    "                res_temp.extend(complete_prediction(10-len(res_temp),self.sender, self.address_books, res_temp))\n",
    "            res.append(res_temp)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predictor KNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Predictor_KNN():\n",
    "    def __init__(self, X, y, sender, address_books,N=10):\n",
    "        self.train = X\n",
    "        self.predict = y.values\n",
    "        self.sender = sender\n",
    "        self.N = min(N,10)\n",
    "        self.address_books = address_books\n",
    "\n",
    "    def prediction(self, X):\n",
    "        res = []\n",
    "        for i in range(X.shape[0]):\n",
    "            cos = cosine_similarity(X[i],self.train).argsort()[:,:30][0] # 30 mails les plus proches\n",
    "            if self.N != 0:\n",
    "                NN_recpt = {}\n",
    "                for j in range(30):\n",
    "                    for k in range(len(self.predict[cos[j]])):\n",
    "                        if self.predict[cos[j]][k] in NN_recpt:\n",
    "                            NN_recpt[self.predict[cos[j]][k]]+= 1\n",
    "                        else:\n",
    "                            NN_recpt[self.predict[cos[j]][k]] = 1\n",
    "                res_temp = [k for k in sorted(NN_recpt, key=NN_recpt.get, reverse=True)][:10]\n",
    "                #list(dict(sorted(NN_recpt.items(), key=operator.itemgetter(1), reverse=True)[:10]))\n",
    "            else:\n",
    "                 res_temp = []\n",
    "            # if less than 10 recipients, complete the prediction with more frequents users\n",
    "            if len(res_temp) < 10:\n",
    "                res_temp.extend(complete_prediction(10-len(res_temp),self.sender, self.address_books, res_temp))\n",
    "            res.append(res_temp)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predictor Tf-Idf centralized centroïd</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "class Predictor_CTFIDF():\n",
    "    def __init__(self, X, y, sender, address_books,N=10, NMost=30):\n",
    "        self.N = min(N,10)\n",
    "        self.train = X\n",
    "        self.predict = y.values\n",
    "        self.sender = sender\n",
    "        self.address_books = address_books\n",
    "        self.X_recpt = {}\n",
    "        \n",
    "        # perform centroid Tf-Idf. i.e each 10 most frequent recipients is represented \n",
    "        # by an average of all mail he received.\n",
    "        # exctract 10 most frequents recipients\n",
    "        self.k_most = [elt[0] for elt in address_books[sender][:NMost]]\n",
    "        # perform average Tf-Idf on 10 most frequents recipients\n",
    "        for recpt in self.k_most: # loop trough 10 most frequents recipients\n",
    "            for i in range(X.shape[0]): # loop trough all mails send by sender\n",
    "                if recpt in self.predict[i]: # if recipients is in mail\n",
    "                    if recpt in self.X_recpt:\n",
    "                        self.X_recpt[recpt] += X[i,:]\n",
    "                    else:\n",
    "                        self.X_recpt[recpt] = X[i,:]\n",
    "            #self.X_recpt[recpt] = normalize(self.X_recpt[recpt], norm='l2', axis=1) # normalize tfidf vector\n",
    "\n",
    "    def prediction(self, X):\n",
    "        res = []\n",
    "        cos = {}\n",
    "        for i in range(X.shape[0]):\n",
    "            # cosine similarity with 10 most frequents recpt\n",
    "            for recpt, value in self.X_recpt.items():\n",
    "                #if isinstance(self.X_recpt[recpt], list): # check that sender has more than 10 recipients\n",
    "                cos[recpt] = cosine_similarity(X[i],self.X_recpt[recpt])#.argsort()[:,0][0]\n",
    "                #else:\n",
    "                #    cos[recpt] = 1\n",
    "            #if i==0:\n",
    "            #    print(cos)\n",
    "            #if 'no.address@enron.com' in cos:\n",
    "            #    cos['no.address@enron.com'] += 100\n",
    "            if self.N != 0:\n",
    "                # return the 10 most frequent recipients in order \n",
    "                # given by similarity to their centroid Tf-Idf representation\n",
    "                res_temp = [k for k in sorted(cos, key=cos.get, reverse=True)][:10]\n",
    "                #list(dict(sorted(cos.items(), key=operator.itemgetter(1), reverse=False)[:10]))\n",
    "            else:\n",
    "                 res_temp = []\n",
    "            # if less than 10 recipients, complete the prediction with more frequents users\n",
    "            if len(res_temp) < 10:\n",
    "                res_temp.extend(complete_prediction(10-len(res_temp),self.sender, self.address_books, res_temp))\n",
    "            res.append(res_temp)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cross validation module</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold\n",
      "------------\n",
      "------------------------------------------------------------------------------------------\n",
      "error TOT = 0.53\n",
      "------------------------------------------------------------------------------------------\n",
      "error TOT = 0.53\n",
      "------------------------------------------------------------------------------------------\n",
      "error TOT = 0.53\n",
      "------------------------------------------------------------------------------------------\n",
      "error TOT = 0.52\n",
      "------------------------------------------------------------------------------------------\n",
      "error TOT = 0.53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-27185841de6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mpdt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPredictor_CTFIDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tfidf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender_train_is\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender_train_is\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msender\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maddress_books\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mpdt_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPredictor_CTFIDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tfidf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender_train_is\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender_train_is\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msender\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maddress_books\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender_test_is\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tfidf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender_test_is\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0my_pred_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender_test_is\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdt_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tfidf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msender_test_is\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1058740aaf2f>\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrecpt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_recpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;31m#if isinstance(self.X_recpt[recpt], list): # check that sender has more than 10 recipients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mcos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrecpt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_recpt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrecpt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.argsort()[:,0][0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[1;31m#else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;31m#    cos[recpt] = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_normalized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \"\"\"\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"toarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[0mmajor_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# convert to this format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         idx_dtype = get_index_dtype((self.indptr, self.indices,\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'to'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;31m###################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guillaume/anaconda3/lib/python3.5/site-packages/scipy/sparse/csc.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    151\u001b[0m                   \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                   \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                   data)\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcsr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "# splitting data for cross validation\n",
    "skf = KFold(n_splits=10, shuffle=True, random_state=None)\n",
    "#print('%10s | %40s | %10s | %10s' %('sender_nb', 'sender', 'accuracy','accuracy freq'))\n",
    "#print('%10s + %40s + %10s + %13s' %(10*'-', 40*'-', 10*'-', 13*'-'))\n",
    "\n",
    "print('K-Fold')\n",
    "print('------------')\n",
    "final_error=0\n",
    "for train_is, test_is in skf.split(y_df):\n",
    "    X_tfidf_train = X_TFIDF[train_is].copy()\n",
    "    y_train = y_df.recipients.loc[train_is].copy()\n",
    "    X_tfidf_test = X_TFIDF[test_is].copy()\n",
    "    y_test = y_df.recipients.loc[test_is].copy()\n",
    "    X_test_df = X_df.loc[test_is].copy()\n",
    "    X_train_df = X_df.loc[train_is].copy()\n",
    "    \n",
    "    i=0\n",
    "    pdt = {}\n",
    "    pdt_freq = {}\n",
    "    accuracy = {}\n",
    "    accuracy_freq = {}\n",
    "    accuracy_TOT = 0\n",
    "    sender_test = X_test_df.sender.unique().tolist()\n",
    "    y_pred = np.empty((X_test_df.shape[0],10),dtype=object)\n",
    "    y_pred_freq = np.empty((X_test_df.shape[0],10),dtype=object)\n",
    "\n",
    "    for sender in sender_test:\n",
    "        #print('%10s | %40s | ' %(sender_test.index(sender), sender), end='')\n",
    "        # indices corresponding to the sender\n",
    "        sender_train_is = np.array(X_train_df.sender == sender)\n",
    "        sender_test_is = np.array(X_test_df.sender == sender)\n",
    "        \n",
    "        pdt[sender] = Predictor_CTFIDF(X_tfidf_train[sender_train_is], y_train[sender_train_is], sender, address_books)\n",
    "        pdt_freq[sender] = Predictor_CTFIDF(X_tfidf_train[sender_train_is], y_train[sender_train_is], sender, address_books, N=0)\n",
    "        y_pred[sender_test_is] = pdt[sender].prediction(X_tfidf_test[sender_test_is])\n",
    "        y_pred_freq[sender_test_is] = pdt_freq[sender].prediction(X_tfidf_test[sender_test_is])\n",
    "        \n",
    "        accuracy[sender] = mapk(y_test[sender_test_is], y_pred[sender_test_is])\n",
    "        accuracy_freq[sender] = mapk(y_test[sender_test_is], y_pred_freq[sender_test_is])\n",
    "        \n",
    "        accuracy_TOT += accuracy[sender]\n",
    "        #print('%10s | %13s' %(round(accuracy[sender],2),round(accuracy_freq[sender],2)))\n",
    "        \n",
    "\n",
    "    print('%30s'%(90*'-'))\n",
    "    print('error TOT = %.2f' %(accuracy_TOT/len(accuracy)))\n",
    "    final_error=final_error+accuracy_TOT/len(accuracy)\n",
    "final_error=final_error/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(y_pred[np.array(X_test_df.sender == 'kimberly.hillis@enron.com')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(y_pred_freq[np.array(X_test_df.sender == 'kimberly.hillis@enron.com')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(y_test[np.array(X_test_df.sender == 'kimberly.hillis@enron.com')].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Create submission</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(y_pred,X_test_df):\n",
    "\n",
    "    predictions_towrite={}\n",
    "    x_test=X_test_df.values\n",
    "    for i in range(len(y_pred)):\n",
    "        recipients=y_pred[i]\n",
    "        mid=x_test[i][0]\n",
    "        predictions_towrite[mid]=recipients\n",
    "\n",
    "    count=0\n",
    "    with open('./pred_custom.txt', 'w') as my_file:\n",
    "        my_file.write('mid,recipients' + '\\n')\n",
    "        for ids, preds in predictions_towrite.items():\n",
    "            count=count+1\n",
    "            r=str(ids)+\",\"\n",
    "            for s in preds:\n",
    "                r=r+\" \"+str(s)\n",
    "            r=r+'\\n'\n",
    "            my_file.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sender_test = X_sub_df.sender.unique().tolist()\n",
    "y_pred = np.empty((X_sub_df.shape[0],10),dtype=object)\n",
    "count=0\n",
    "pdt = {}\n",
    "\n",
    "for sender in sender_test:\n",
    "    # indices corresponding to the sender\n",
    "    sender_train_is = np.array(X_df.sender == sender)\n",
    "    sender_test_is = np.array(X_sub_df.sender == sender)\n",
    "    \n",
    "    y_train_all = y_df.recipients.copy()\n",
    "    pdt[sender] = Predictor_CTFIDF(X_TFIDF[sender_train_is], y_train_all[sender_train_is], sender, address_books)\n",
    "    y_pred[sender_test_is] = pdt[sender].prediction(X_sub_TFIDF[sender_test_is])\n",
    "    \n",
    "create_submission(y_pred,X_sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
